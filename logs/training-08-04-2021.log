2021-04-08 17:32:11.450160: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-08 17:32:11.450240: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Loading BERT Model for sequence classification
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading BERT-Tokenizer...
Tokenizing segments...
100%|████████████████████████████████████████████████████████████████████| 6460/6460 [00:03<00:00, 1728.57it/s]
Padding/truncating all sentences to 180 values...
Padding token: '[PAD]',ID: {self.tokenizer.pad_token_id}

======== Epoch 1 / 4 ========
Training...
train.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  outputs = self.model(torch.tensor(b_input_ids).to(
pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp  Batch    40  of    182.    Elapsed: 0:30:48.
  Batch    80  of    182.    Elapsed: 0:47:56.
  Batch   120  of    182.    Elapsed: 1:07:05.
  Batch   160  of    182.    Elapsed: 1:25:47.

  Average training loss: 0.68
  Training epoch took: 1:34:57

Running Validation...
  Accuracy: 0.81
  Validation took: 0:02:52

======== Epoch 2 / 4 ========
Training...
  Batch    40  of    182.    Elapsed: 0:25:33.
  Batch    80  of    182.    Elapsed: 0:42:18.
  Batch   120  of    182.    Elapsed: 0:59:44.
  Batch   160  of    182.    Elapsed: 1:16:57.

  Average training loss: 0.38
  Training epoch took: 1:26:42

Running Validation...
  Accuracy: 0.80
  Validation took: 0:02:56

======== Epoch 3 / 4 ========
Training...
  Batch    40  of    182.    Elapsed: 0:27:08.
  Batch    80  of    182.    Elapsed: 0:44:01.
  Batch   120  of    182.    Elapsed: 1:00:48.
  Batch   160  of    182.    Elapsed: 1:18:49.

  Average training loss: 0.25
  Training epoch took: 1:27:59

Running Validation...
  Accuracy: 0.82
  Validation took: 0:02:57

======== Epoch 4 / 4 ========
Training...
  Batch    40  of    182.    Elapsed: 0:27:19.
  Batch    80  of    182.    Elapsed: 0:43:56.
  Batch   120  of    182.    Elapsed: 1:00:39.
  Batch   160  of    182.    Elapsed: 1:17:48.

  Average training loss: 0.17
  Training epoch took: 1:27:55

Running Validation...
  Accuracy: 0.84
  Validation took: 0:02:56
Training complete!
